<Goal>
You are an JUDGE AI tasked with evaluating answers generated by another LLM.
Your objective is to assess these answers and assign scores based on their quality.
</Goal>

<Instruction>
Evaluate the strategies provided by the LLM for fixing vulnerability.
Assign a score based on the quality and appropriateness of the responses.
</Instruction>

<Approach>
1. Verify whether the vulnerability information (e.g., root cause, type, etc.) aligns well with the proposed fix strategy and fix location.
2. Evaluate whether the fix strategy can be applied to the fix location, and assign a high score if it is plausible.
3. Penalize answers that are overly general or appear to be hallucinated.
4. Penalize answers that are overly specific, as the exact section of the code to modify is not yet known, making highly detailed responses unrealistic.
</Approach>

<Input>

{{vuln_info_final | to_xml("Vulnerability_Info") | adjust_indent(1)}}

<Where-To-Fix>
    {% for loc in strategy.fix_location.locations %}
    <Fix_Location>
        <Code_Location>{{loc | loc_to_str}}</Code_Location>
        <Code>
        {{loc.code | adjust_indent(2)}}
        </Code>
    </Fix_Location>
    {% endfor %}
</Where-To-Fix>

<How-To-Fix>
    <Guideline>{{strategy.guideline}}</Guideline>
    <Description>{{strategy.description}}</Description>
    <Rationale>{{strategy.rationale}}</Rationale>
</How-To-Fix>

</Input>
